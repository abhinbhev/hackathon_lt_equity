{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 200)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: \"%.3f\" % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Synergy_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country='korea'\n",
    "brand='cass'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vehicle Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles = ['digitaldisplayandsearch', 'facebook', 'instagram', 'programmaticvideo', 'youtube']  ## for korea\n",
    "# vehicles = ['digitaldisplayandsearch', 'digitalvideo', 'tv', 'ott', 'otv', 'ooh'] ## for china\n",
    "# vehicles = ['digitaldisplayandsearch','digitalvideo','facebook','instagram','ooh', 'opentv', 'paytv', 'print', 'programatic_display', 'programatic_video', 'programmatic', 'radio', 'twitter', 'youtube'] ## for peru\n",
    "# vehicles = [] ## for mexico\n",
    "# vehicles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = pd.read_csv(\"raw_data/South Korea Q3 Model/mapping.csv\")\n",
    "vehicles = vehicles.loc[vehicles['Harmonized'] != 'REMOVE']\n",
    "vehicles = vehicles['vehicle'].tolist()\n",
    "vehicles.sort()\n",
    "vehicles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Synergy_Model(country=country, brand=brand, vehicles = vehicles)\n",
    "# dataset = Synergy_Model(country=country, brand=brand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.data_creation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synergy Variable Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_cols = [x for x in data.columns if '_volume' in x]\n",
    "data['Total'] = data[volume_cols].sum(axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in dataset.pairs:\n",
    "    # data[pair[0] + \"x\" + pair[1] + '_spend'] = np.sqrt(np.sqrt(data[pair[0]+'_spend'] * data[pair[1]+'_spend']))\n",
    "    data[pair[0] + \"x\" + pair[1] + '_spend'] = np.sqrt(data[pair[0]+'_spend'] * data[pair[1]+'_spend'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.loc[data['year'] == 2022]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaGammaDecay:\n",
    "    def __init__(self, x, beta, gamma, num_dates, num_vehicles):\n",
    "        self.impact_by_signal_instant = x\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.num_vehicles = num_vehicles\n",
    "\n",
    "        self.impact_by_signal_decayed = decayed_signal = self.impact_by_signal_instant * self.beta\n",
    "\n",
    "        # self.num_dates = min(x.shape[0], num_dates) \n",
    "        for i in range(num_dates):\n",
    "            gamma = self.gamma\n",
    "            decayed_signal = tf.concat(\n",
    "                (\n",
    "                    tf.zeros(shape=(1, self.num_vehicles), dtype=tf.float32),\n",
    "                    decayed_signal[:-1] * gamma,\n",
    "                ),\n",
    "                0,\n",
    "            )\n",
    "            self.impact_by_signal_decayed += decayed_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class new_Synergy():\n",
    "    def __init__(self, input_shape, data_input, data_target, num_significant, aux_data):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_significant = num_significant\n",
    "        self.np_input = data_input.to_numpy().reshape(self.input_shape[0], self.input_shape[1])\n",
    "        self.np_target = data_target.to_numpy().reshape(self.input_shape[0], 1)\n",
    "        self.np_aux = aux_data.to_numpy().reshape(self.input_shape[0], self.num_significant)\n",
    "        self.create_placeholders()        \n",
    "        self.create_variables()\n",
    "        self.create_infra()\n",
    "        self.train_op = tf.compat.v1.train.GradientDescentOptimizer(learning_rate = 1).minimize(self.loss)\n",
    "        self.feed_dict = {\n",
    "            self.ph_target:self.np_target,\n",
    "            self.ph_input:self.np_input,\n",
    "            self.ph_aux:self.np_aux\n",
    "        }\n",
    "\n",
    "\n",
    "    def create_placeholders(self):\n",
    "        self.ph_input = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, self.input_shape[1]))\n",
    "        self.ph_target = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 1))\n",
    "        self.ph_aux = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, self.num_significant))\n",
    "    \n",
    "\n",
    "    def create_variables(self):\n",
    "        # gamma\n",
    "        gamma_initial = np.zeros(shape = (1, self.input_shape[-1])).astype(np.float32)\n",
    "        self.gamma_raw = tf.Variable(gamma_initial, trainable=True, name = 'decay_gamma_raw')\n",
    "\n",
    "        # beta\n",
    "        beta_initial_sig = np.zeros(shape = (1, self.num_significant)).astype(np.float32)\n",
    "        self.beta_raw_sig = tf.Variable(beta_initial_sig, trainable=True, name = 'decay_beta_raw_sig')\n",
    "        beta_initial_syn = np.zeros(shape = (1, self.input_shape[-1] - self.num_significant)).astype(np.float32)\n",
    "        self.beta_raw_syn = tf.Variable(beta_initial_syn, trainable=True, name = 'decay_beta_raw_syn')\n",
    "\n",
    "        # sigmoid curve a\n",
    "        initial_a_sig = np.zeros(shape = (1, self.num_significant)).astype(np.float32)\n",
    "        self.a_raw_sig = tf.Variable(initial_a_sig, trainable=True, name = 'sigmoid_a_raw_sig')\n",
    "        initial_a_syn = np.zeros(shape = (1, self.input_shape[-1] - self.num_significant)).astype(np.float32)\n",
    "        self.a_raw_syn = tf.Variable(initial_a_syn, trainable=True, name = 'sigmoid_a_raw_syn')\n",
    "\n",
    "        # sigmoid curve b\n",
    "        initial_b = np.zeros(shape = (1, self.input_shape[-1])).astype(np.float32)\n",
    "        self.b_raw = tf.Variable(initial_b, trainable=True, name = 'sigmoid_b_raw')\n",
    "\n",
    "    \n",
    "    def create_infra(self):\n",
    "        self.init = tf.compat.v1.global_variables_initializer()\n",
    "        with tf.compat.v1.variable_scope(\"synergy_volumes\"):\n",
    "            # beta-gamma\n",
    "            self.gamma = tf.nn.sigmoid(self.gamma_raw, name = 'decay_gamma') * 0.3 + 0.5 \n",
    "            self.beta_sig = tf.nn.sigmoid(self.beta_raw_sig, name = 'decay_beta_sig') * 0.3 + 0.5\n",
    "            self.beta_syn = tf.nn.sigmoid(self.beta_raw_syn, name = 'decay_beta_syn') * 0.3 + 0.5\n",
    "            self.beta = tf.concat(\n",
    "                (\n",
    "                    self.beta_sig,\n",
    "                    self.beta_syn,\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "            self.decayed_impact = BetaGammaDecay(self.ph_input, \n",
    "                                                beta = self.beta, \n",
    "                                                gamma = self.gamma, \n",
    "                                                num_dates=self.np_input.shape[0], \n",
    "                                                num_vehicles=self.ph_input.shape[1]).impact_by_signal_decayed\n",
    "            self.decayed_impact += self.ph_input\n",
    "            # sigmoid curve\n",
    "            self.a_sig = tf.nn.sigmoid(self.a_raw_sig, name = 'sigmoid_a') * 4 + 0.1\n",
    "            self.a_syn = tf.nn.sigmoid(self.a_raw_syn, name = 'sigmoid_a') * 0.05 + 0.001\n",
    "            self.a = tf.concat(\n",
    "                (\n",
    "                    self.a_sig,\n",
    "                    self.a_syn,\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "            self.b = tf.nn.sigmoid(self.b_raw, name = 'sigmoid_b') * 3 + 1\n",
    "            self.x_start = 0.7\n",
    "            B_slope_multiplier = 0.01\n",
    "            self.shifted_intercept = tf.constant(np.log(3) - np.log(7), dtype=tf.float32)\n",
    "            offset = (self.a / self.b) * self.shifted_intercept\n",
    "            self.curve_impact = self.a * ((1 / (1 + tf.exp(-(self.b / (self.a)) * (self.decayed_impact - offset)))) - self.x_start) + self.decayed_impact * self.b * B_slope_multiplier\n",
    "            \n",
    "            self.aux_loss = tf.sqrt(tf.reduce_mean(tf.square(tf.maximum(self.curve_impact[:,:self.num_significant] - self.ph_aux, 0))))\n",
    "            self.aux_loss1 = tf.sqrt(tf.reduce_mean(tf.square(self.curve_impact[:,:self.num_significant] - self.ph_aux)))\n",
    "            self.aux_loss2 = tf.sqrt(tf.reduce_mean(tf.square(tf.maximum(tf.reduce_sum(self.curve_impact[:,self.num_significant:])/tf.reduce_sum(self.curve_impact[:,:self.num_significant]) - 0.5, 0))))\n",
    "            self.yhat = tf.reduce_sum(self.curve_impact, axis = 1, keepdims=True)\n",
    "            self.loss = tf.sqrt(tf.reduce_mean(tf.square(self.yhat - self.ph_target))) + self.aux_loss2 / 5\n",
    "            # self.loss = tf.sqrt(tf.reduce_mean(tf.square(self.yhat - self.ph_target))) + 10*self.aux_loss + 10*(self.aux_loss1) + (self.aux_loss2)\n",
    "\n",
    "\n",
    "    \n",
    "    def _train(self, num_epochs):\n",
    "        self.cost_trace = []\n",
    "        self.sess_ = tf.compat.v1.Session()\n",
    "        self.sess_.run(self.init)\n",
    "        progress = tqdm(range(num_epochs))\n",
    "        for i in range(num_epochs):\n",
    "            _, cur_loss = self.sess_.run([self.train_op, self.loss], self.feed_dict)\n",
    "            self.cost_trace.append(cur_loss)\n",
    "            progress.set_postfix(loss=np.average(self.cost_trace))\n",
    "            progress.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vehicle_volumes = {}\n",
    "all_vehicle_weights = {}\n",
    "for vehicle in dataset.significant_vehicles:\n",
    "    # vehicle = 'YOUTUBE'\n",
    "    print(vehicle)\n",
    "    spend_cols = []\n",
    "    spend_cols.append(vehicle + '_spend')\n",
    "    for pair in dataset.pairs:\n",
    "        if vehicle in pair:\n",
    "            spend_cols.append(pair[0] + \"x\" + pair[1] + '_spend')\n",
    "    max_spend = data[spend_cols].max().max()\n",
    "    input_data = data[spend_cols].copy()/max_spend\n",
    "    target_data = data[vehicle + '_volume'].copy()/data[vehicle + '_volume'].max()\n",
    "    aux_data = data[vehicle + '_volume'].copy()/data[vehicle + '_volume'].max()\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = new_Synergy(input_shape=(data.shape[0],len(dataset.significant_vehicles)), \n",
    "                        data_input= input_data,\n",
    "                        data_target= target_data,\n",
    "                        num_significant=1,\n",
    "                        aux_data=aux_data\n",
    "                    )\n",
    "    model._train(1000)\n",
    "\n",
    "    results = pd.DataFrame(data={'y_true':model.np_target.reshape(-1), 'y_pred':model.sess_.run(model.yhat, model.feed_dict).reshape(-1)})\n",
    "    results *= data['Total'].max()\n",
    "    results.plot()\n",
    "    plt.show()\n",
    "    r2 = r2_score(results['y_true'], results['y_pred'])\n",
    "    print(r2)\n",
    "\n",
    "    volumes = pd.DataFrame(model.sess_.run(model.curve_impact, model.feed_dict), columns = [x[:-5]+'volume' for x in spend_cols]) * data[vehicle + '_volume'].max()\n",
    "    volumes = volumes * data[vehicle+'_volume'].sum() / volumes.sum().sum()\n",
    "    all_vehicle_volumes[vehicle] = volumes\n",
    "    breakdown = volumes.sum() / data[vehicle+'_volume'].sum() * 100\n",
    "    display(breakdown)\n",
    "    weights = {\n",
    "        'sigmoid_a' : model.sess_.run(model.a, model.feed_dict),\n",
    "        'sigmoid_b' : model.sess_.run(model.b, model.feed_dict),\n",
    "        'decay_beta' : model.sess_.run(model.beta, model.feed_dict),\n",
    "        'decay_gamma' : model.sess_.run(model.gamma, model.feed_dict),\n",
    "    }\n",
    "    print(weights)\n",
    "    all_vehicle_weights[vehicle] = weights\n",
    "\n",
    "    print()\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Level "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_aggregate = np.zeros(shape=(len(dataset.significant_vehicles), len(dataset.significant_vehicles)))\n",
    "incoming_aggregate = pd.DataFrame(incoming_aggregate, columns=dataset.significant_vehicles, index=dataset.significant_vehicles)\n",
    "\n",
    "for vehicle in dataset.significant_vehicles:\n",
    "    temp = all_vehicle_volumes[vehicle].copy()\n",
    "    incoming_aggregate.loc[vehicle][vehicle] = temp[vehicle+'_volume'].sum().sum()\n",
    "    for col in temp.columns:\n",
    "        if col != vehicle+'_volume':\n",
    "            other_veh = col[:-7].split('x')\n",
    "            other_veh.remove(vehicle)\n",
    "            other_veh = other_veh[0]\n",
    "            incoming_aggregate.loc[vehicle][other_veh] = temp[col].sum().sum()\n",
    "\n",
    "incoming_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_rows = incoming_aggregate.sum(axis=1)\n",
    "sum_of_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoing_aggregate = incoming_aggregate.copy().T\n",
    "for vehicle in dataset.significant_vehicles:\n",
    "    outgoing_aggregate.loc[vehicle][vehicle] = sum_of_rows[vehicle] - outgoing_aggregate.loc[vehicle].sum() + outgoing_aggregate.loc[vehicle][vehicle]\n",
    "    \n",
    "outgoing_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoing_aggregate.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1_aggregate = pd.DataFrame(data = np.zeros(shape=(len(dataset.significant_vehicles), 2)), columns=['self', 'incoming'], index=dataset.significant_vehicles)\n",
    "for vehicle in dataset.significant_vehicles:\n",
    "    chart_1_aggregate.loc[vehicle]['self'] = incoming_aggregate.loc[vehicle][vehicle]\n",
    "    chart_1_aggregate.loc[vehicle]['incoming'] = incoming_aggregate.loc[vehicle].sum() - incoming_aggregate.loc[vehicle][vehicle]\n",
    "chart_1_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1_pp_aggregate = chart_1_aggregate.copy()\n",
    "# divide each row by the sum of the row\n",
    "for vehicle in dataset.significant_vehicles:\n",
    "    if chart_1_pp_aggregate.loc[vehicle].sum() != 0:\n",
    "        chart_1_pp_aggregate.loc[vehicle] /= chart_1_pp_aggregate.loc[vehicle].sum()\n",
    "    else:\n",
    "        chart_1_pp_aggregate.loc[vehicle] = 0\n",
    "chart_1_pp_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_2_aggregate = None\n",
    "for vehicle in dataset.significant_vehicles:\n",
    "    temp = []\n",
    "    temp.append([vehicle]*(len(dataset.significant_vehicles)-1)*2)\n",
    "    temp.append(['Outgoing']*(len(dataset.significant_vehicles)-1) + ['Incoming']*(len(dataset.significant_vehicles)-1))\n",
    "    temp.append([x for x in dataset.significant_vehicles if x != vehicle]*2)\n",
    "    temp.append(list(outgoing_aggregate.loc[vehicle][[x for x in dataset.significant_vehicles if x != vehicle]].values) + list(incoming_aggregate.loc[vehicle][[x for x in dataset.significant_vehicles if x != vehicle]].values))\n",
    "    if chart_2_aggregate is None:\n",
    "        chart_2_aggregate = pd.DataFrame(np.array(temp).transpose())\n",
    "    else:\n",
    "        chart_2_aggregate = pd.concat([chart_2_aggregate, pd.DataFrame(np.array(temp).transpose())])\n",
    "chart_2_aggregate.columns = ['Vehicle', 'Direction', 'Other Vehicle', 'Volume']\n",
    "chart_2_aggregate['Volume'] = chart_2_aggregate['Volume'].astype(float).apply(lambda x: round(x, 2))\n",
    "chart_2_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yearly Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data, find the index range of each year\n",
    "year_ranges = {}\n",
    "for year in data['year'].unique():\n",
    "    year_ranges[year] = (data[data['year'] == year].index[0], data[data['year'] == year].index[-1] + 1)\n",
    "year_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_ranges.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incoming_yearly = {}\n",
    "for year in year_ranges.keys():\n",
    "    incoming = np.zeros(shape=(len(dataset.significant_vehicles), len(dataset.significant_vehicles)))\n",
    "    incoming = pd.DataFrame(incoming, columns=dataset.significant_vehicles, index=dataset.significant_vehicles)\n",
    "    for vehicle in dataset.significant_vehicles:\n",
    "        temp = all_vehicle_volumes[vehicle].copy()\n",
    "        temp = temp[year_ranges[year][0]:year_ranges[year][1]]\n",
    "        \n",
    "        incoming.loc[vehicle][vehicle] = temp[vehicle+'_volume'].sum().sum()\n",
    "        for col in temp.columns:\n",
    "            if col != vehicle+'_volume':\n",
    "                other_veh = col[:-7].split('x')\n",
    "                other_veh.remove(vehicle)\n",
    "                other_veh = other_veh[0]\n",
    "                incoming.loc[vehicle][other_veh] = temp[col].sum().sum()\n",
    "    incoming_yearly[year] = incoming\n",
    "display(incoming_yearly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### THIS IS JUST A VERIFICATION! It should match with aggregate. #######\n",
    "\n",
    "incoming_yearly_sum = pd.DataFrame(np.zeros(shape=(len(dataset.significant_vehicles), len(dataset.significant_vehicles))), columns=dataset.significant_vehicles, index=dataset.significant_vehicles)\n",
    "for year in incoming_yearly.keys():\n",
    "    incoming_yearly_sum += incoming_yearly[year]\n",
    "incoming_yearly_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoing_yearly = {}\n",
    "for year in year_ranges.keys():\n",
    "    outgoing = incoming_yearly[year].copy().T\n",
    "    for vehicle in dataset.significant_vehicles:\n",
    "        outgoing.loc[vehicle][vehicle] = incoming_yearly[year].loc[vehicle].sum() - outgoing.loc[vehicle].sum() + outgoing.loc[vehicle][vehicle]\n",
    "    outgoing_yearly[year] = outgoing\n",
    "outgoing_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### THIS IS JUST A VERIFICATION! It should match with aggregate. #######\n",
    "\n",
    "outgoing_yearly_sum = pd.DataFrame(np.zeros(shape=(len(dataset.significant_vehicles), len(dataset.significant_vehicles))), columns=dataset.significant_vehicles, index=dataset.significant_vehicles)\n",
    "for year in outgoing_yearly.keys():\n",
    "    outgoing_yearly_sum += outgoing_yearly[year]\n",
    "outgoing_yearly_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1_yearly = {}\n",
    "for year in year_ranges.keys():\n",
    "    chart_1 = pd.DataFrame(data = np.zeros(shape=(len(dataset.significant_vehicles), 2)), columns=['self', 'incoming'], index=dataset.significant_vehicles)\n",
    "    for vehicle in dataset.significant_vehicles:\n",
    "        chart_1.loc[vehicle]['self'] = incoming_yearly[year].loc[vehicle][vehicle]\n",
    "        chart_1.loc[vehicle]['incoming'] = incoming_yearly[year].loc[vehicle].sum() - incoming_yearly[year].loc[vehicle][vehicle]\n",
    "    chart_1_yearly[year] = chart_1\n",
    "chart_1_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_1_pp_yearly = {}\n",
    "for year in year_ranges.keys():\n",
    "    chart_1_pp = chart_1_yearly[year].copy()\n",
    "    for vehicle in dataset.significant_vehicles:\n",
    "        if chart_1_pp.loc[vehicle].sum() != 0:\n",
    "            chart_1_pp.loc[vehicle] /= chart_1_pp.loc[vehicle].sum()\n",
    "        else:\n",
    "            chart_1_pp.loc[vehicle] = 0\n",
    "    chart_1_pp_yearly[year] = chart_1_pp\n",
    "chart_1_pp_yearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_2_yearly = {}\n",
    "for year in year_ranges.keys():\n",
    "    chart_2 = None\n",
    "    for vehicle in dataset.significant_vehicles:\n",
    "        temp = []\n",
    "        temp.append([vehicle]*(len(dataset.significant_vehicles)-1)*2)\n",
    "        temp.append(['Outgoing']*(len(dataset.significant_vehicles)-1) + ['Incoming']*(len(dataset.significant_vehicles)-1))\n",
    "        temp.append([x for x in dataset.significant_vehicles if x != vehicle]*2)\n",
    "        temp.append(list(outgoing_aggregate.loc[vehicle][[x for x in dataset.significant_vehicles if x != vehicle]].values) + list(incoming_aggregate.loc[vehicle][[x for x in dataset.significant_vehicles if x != vehicle]].values))\n",
    "        if chart_2 is None:\n",
    "            chart_2 = pd.DataFrame(np.array(temp).transpose())\n",
    "        else:\n",
    "            chart_2 = pd.concat([chart_2, pd.DataFrame(np.array(temp).transpose())])\n",
    "    chart_2.columns = ['Vehicle', 'Direction', 'Other Vehicle', 'Volume']\n",
    "    chart_2['Volume'] = chart_2['Volume'].astype(float).apply(lambda x: round(x, 2))\n",
    "    chart_2_yearly[year] = chart_2\n",
    "chart_2_yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump the Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'models_ml_split_eqn/' + country + '_' + brand + '.pkl'\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump(all_vehicle_weights, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump All Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'results_ml_split_eqn/' + country + '_' + brand + '.pkl'\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    "    pickle.dump([incoming_aggregate, outgoing_aggregate, chart_1_aggregate, chart_1_pp_aggregate, chart_2_aggregate, incoming_yearly, outgoing_yearly, chart_1_yearly, chart_1_pp_yearly, chart_2_yearly], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'results_ml_split_eqn/' + country + '_' + brand + '.xlsx'\n",
    "data.to_excel(file_path, sheet_name='data', index=False)\n",
    "with pd.ExcelWriter(file_path, engine = 'openpyxl', mode = 'a') as writer:\n",
    "    incoming_aggregate.to_excel(writer, sheet_name='incoming_aggregate')\n",
    "    outgoing_aggregate.to_excel(writer, sheet_name='outgoing_aggregate')\n",
    "    chart_1_aggregate.to_excel(writer, sheet_name='chart_1_aggregate')\n",
    "    chart_1_pp_aggregate.to_excel(writer, sheet_name='chart_1_pp_aggregate')\n",
    "    chart_2_aggregate.to_excel(writer, sheet_name='chart_2_aggregate')\n",
    "    incoming_yearly_concat = pd.DataFrame()\n",
    "    for year in incoming_yearly.keys():\n",
    "        temp = incoming_yearly[year].copy()\n",
    "        temp.reset_index(inplace=True)\n",
    "        temp.rename(columns={'index':year}, inplace=True)\n",
    "        incoming_yearly_concat = pd.concat([incoming_yearly_concat, temp], axis=1)\n",
    "    incoming_yearly_concat.to_excel(writer, sheet_name='incoming_yearly')\n",
    "    outgoing_yearly_concat = pd.DataFrame()\n",
    "    for year in outgoing_yearly.keys():\n",
    "        temp = outgoing_yearly[year].copy()\n",
    "        temp.reset_index(inplace=True)\n",
    "        temp.rename(columns={'index':year}, inplace=True)\n",
    "        outgoing_yearly_concat = pd.concat([outgoing_yearly_concat, temp], axis=1)\n",
    "    outgoing_yearly_concat.to_excel(writer, sheet_name='outgoing_yearly')\n",
    "    chart_1_yearly_concat = pd.DataFrame()\n",
    "    for year in chart_1_yearly.keys():\n",
    "        temp = chart_1_yearly[year].copy()\n",
    "        temp.reset_index(inplace=True)\n",
    "        temp.rename(columns={'index':year}, inplace=True)\n",
    "        chart_1_yearly_concat = pd.concat([chart_1_yearly_concat, temp], axis=1)\n",
    "    chart_1_yearly_concat.to_excel(writer, sheet_name='chart_1_yearly')\n",
    "    chart_1_pp_yearly_concat = pd.DataFrame()\n",
    "    for year in chart_1_pp_yearly.keys():\n",
    "        temp = chart_1_pp_yearly[year].copy()\n",
    "        temp.reset_index(inplace=True)\n",
    "        temp.rename(columns={'index':year}, inplace=True)\n",
    "        chart_1_pp_yearly_concat = pd.concat([chart_1_pp_yearly_concat, temp], axis=1)\n",
    "    chart_1_pp_yearly_concat.to_excel(writer, sheet_name='chart_1_pp_yearly')\n",
    "    chart_2_yearly_concat = pd.DataFrame()\n",
    "    for year in chart_2_yearly.keys():\n",
    "        temp = chart_2_yearly[year].copy()\n",
    "        temp.reset_index(inplace=True)\n",
    "        temp.rename(columns={'index':year}, inplace=True)\n",
    "        chart_2_yearly_concat = pd.concat([chart_2_yearly_concat, temp], axis=1)\n",
    "    chart_2_yearly_concat.to_excel(writer, sheet_name='chart_2_yearly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
